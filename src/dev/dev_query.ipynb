{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def __check_input(start_date, end_date, days, hours):   # Actually does more than just checking input, it also transforms it. Perhaps find a better name?\n",
    "    format = \"%Y-%m-%d\"\n",
    "\n",
    "    # Handle input for \"start_date\" and \"end_date\" \n",
    "    try:\n",
    "        start_date = datetime.strptime(start_date, format)\n",
    "    except ValueError as ve:\n",
    "        raise ValueError('Invalid start date: {0}. Make sure it follows the expected format: yyyy-mm-dd and uses actual calender dates.'.format(start_date)) from ve\n",
    "\n",
    "    try:\n",
    "        end_date = datetime.strptime(end_date, format)\n",
    "    except ValueError as ve:\n",
    "        raise ValueError('Invalid end date: {0}. Make sure it follows the expected format: yyyy-mm-dd and uses actual calender dates.'.format(end_date)) from ve\n",
    "\n",
    "    if end_date < start_date:\n",
    "        raise ValueError('End date {0} is set earlier than start date {1}'.format(end_date.strftime(\"%Y-%m-%d\"), start_date.strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "\n",
    "\n",
    "    # Handle input for \"days\"\n",
    "    days_pattern = re.compile(r'^[a-zA-Z]{3}(, ?[a-zA-Z]{3})*$')\n",
    "    if not days_pattern.match(days):\n",
    "        raise ValueError('The input string for days: \"{0}\" is not in a valid format. Please provide as a string of comma separated days. Example: \"mon, tue, wed\"'.format(days))\n",
    "\n",
    "    days_list = [day.strip().lower() for day in days.split(',')]\n",
    "    valid_days = [\"mon\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun\", \"all\"]\n",
    "\n",
    "    for day in days_list:\n",
    "        if day not in valid_days:\n",
    "            raise ValueError('The given day \"{0}\" is not a valid day. Please use the standard three letter abbreviations: mon, tue, wed, thu, fri, sat, sun, or all.'.format(day, days))\n",
    "\n",
    "    if \"all\" in days_list and len(days_list) > 1: \n",
    "        raise ValueError('Use of \"all\" together with other specified days: {0}. Option \"all\" should be used alone.'.format(days))\n",
    "\n",
    "    day_map = {day: i for i, day in enumerate(valid_days)}\n",
    "    \n",
    "    if \"all\" in days_list:\n",
    "        days_list = [day_map[day] for day in valid_days]\n",
    "    else:\n",
    "        days_list = [day_map[day] for day in days_list]\n",
    "\n",
    "\n",
    "\n",
    "    # Handle input for \"hours\"\n",
    "    hours_pattern = re.compile(r'^\\d{2}-\\d{2}(, ?\\d{2}-\\d{2})*$')\n",
    "    if not hours_pattern.match(hours):\n",
    "        raise ValueError('The input string for hours: \"{0}\" is not in a valid format. Please provide as a string of comma separated ranges. Example: \"06-09, 13-15\"'.format(hours))\n",
    "\n",
    "    hours_list = []\n",
    "    valid_hours = [str(i).zfill(2) for i in range(0,25)]\n",
    "\n",
    "    for h in [h.strip() for h in hours.split(',')]:\n",
    "        start, end = h.split('-')\n",
    "        if end <= start: \n",
    "            raise ValueError('Wrong order of hours: \"{0}\" The first hour in any range should be strictly smaller than the second'.format(h))\n",
    "        if start not in valid_hours or end not in valid_hours:\n",
    "            raise ValueError('Make sure both start \"{0}\" and end \"{1}\" are valid hours of the day.'.format(start, end))\n",
    "        hours_list += [str(day).zfill(2) for day in range(int(start), int(end))]\n",
    "    \n",
    "    hours_list.sort()\n",
    "\n",
    "    if len(hours_list) != len(set(hours_list)):\n",
    "        raise ValueError(\"Make sure that the ranges in hours: {0} do not overlap\".format(hours))\n",
    "\n",
    "    \n",
    "    return start_date, end_date, days_list, hours_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/74113035/fastest-way-to-search-many-files-in-many-directories\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def __process_directories(search_paths, files):\n",
    "\n",
    "    search_paths = [path for path in search_paths if os.path.isdir(path)]\n",
    "\n",
    "    def process_directory(dir):\n",
    "        output = []\n",
    "        for file in os.listdir(dir):\n",
    "            if file in files:\n",
    "                output.append(os.path.join(dir, file))\n",
    "        return output\n",
    "\n",
    "    result = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for rv in executor.map(process_directory, search_paths):\n",
    "            result.extend(rv)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def __findData(start_date, end_date, days, hours):\n",
    "    mind_path = \"/mnt/mind-data\"\n",
    "    paths = []\n",
    "\n",
    "    start_date, end_date, days, hours = __check_input(start_date, end_date, days, hours)\n",
    "\n",
    "    if days == \"all\":\n",
    "        dates = [date for date in [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]]\n",
    "    else:\n",
    "        dates = [date for date in [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)] if date.weekday() in days]\n",
    "    \n",
    "    for date in dates:\n",
    "        search_directories = [os.path.join(mind_path, datetime.strftime(date + timedelta(days=i), \"%y%m%d\")) for i in range(10)]\n",
    "\n",
    "        files = ['DW_{0}_{1}.dat.gz'.format(datetime.strftime(date, \"%Y%m%d\"), hour) for hour in hours]\n",
    "\n",
    "        result = __process_directories(search_directories, files)\n",
    "\n",
    "        paths.extend(result)\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "def createSparkSession(appName = \"calista-analytics\", executor_instances = \"20\", executor_cores = \"3\", executor_memory = \"12g\", master = \"spark://n1:7077\"):\n",
    "\n",
    "    # Add check to see if cluster is up and running with suggestions for ways to start it up\n",
    "\n",
    "    # Add check to see if another application is running and using up resources with suggestions for ways to turn it off\n",
    "\n",
    "    builder = SparkSession.builder.appName(appName) \\\n",
    "    .config(\"spark.executor.instances\", executor_instances).config(\"spark.executor.cores\", executor_cores).config(\"spark.executor.memory\", executor_memory) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .master(master)\n",
    "\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/xadmin/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/xadmin/.ivy2/cache\n",
      "The jars for the packages stored in: /home/xadmin/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2e2b75bf-9630-4195-8dec-a192e51259f0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 131ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2e2b75bf-9630-4195-8dec-a192e51259f0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/25 17:26:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/25 17:26:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = createSparkSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "    \n",
    "    def __getSchema():\n",
    "        headers = [\"USAGE_DTTM\", \"SCRAMBLED_IMSI\", \"Download_Type\", \"MCC\", \"MNC\", \"LAC\", \"SAC\", \"TAC\",\n",
    "                    \"E_NODE_B_ID\", \"SECTOR_ID\", \"RAT\", \"NEW_CALL_ATTEMPTS\", \"NEW_ANSWERED_CALLS\",\n",
    "                    \"NEW_FAILED_CALLS\", \"NEW_DROPPED_CALLS\", \"NEW_PS_ATTEMPTS\", \"NEW_FAILED_PS\", \"NEW_DROPPED_PS\",\n",
    "                    \"NEW_SMS_ATTEMPTS\", \"NEW_FAILED_SMS\", \"NEW_LU_ATTEMPTS\", \"NEW_FAILED_LU\", \"NEW_RAUTAU_ATTEMPTS\",\n",
    "                    \"NEW_FAILED_RAUTAU\", \"NEW_ATTACH_ATTEMPTS\", \"NEW_FAILED_ATTACH\", \"NEW_DETACH_ATTEMPTS\",\n",
    "                    \"NEW_DL_VOLUME\", \"NEW_DL_MAX_THROUGHPUT\", \"NEW_UL_VOLUME\", \"NEW_UL_MAX_THROUGHPUT\"]\n",
    "\n",
    "        fields = [StructField(field_name, StringType(), True) \n",
    "                    if field_name in [\"USAGE_DTTM\", \"SCRAMBLED_IMSI\", \"Download_Type\"] \n",
    "                    else StructField(field_name, IntegerType(), True) \n",
    "                    for field_name in headers]\n",
    "\n",
    "        schema = StructType(fields)\n",
    "\n",
    "        return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "def __ingest(files, batch_size=10):\n",
    "    batches = [files[i:i+batch_size] for i in range(0, len(files), batch_size)]\n",
    "    schema = __getSchema()\n",
    "    dfs = []\n",
    "\n",
    "    for batch in tqdm(batches, desc=\"Processing batches\"):\n",
    "        df = spark.read.csv(batch, schema=schema, sep='\\t') \\\n",
    "        .withColumn(\"timestamp\", to_timestamp(\"USAGE_DTTM\", \"ddMMMyyy:HH:mm:ss\")) \\\n",
    "        .withColumn(\"date\", date_format(\"timestamp\", \"yyyy-MM-dd\")) \\\n",
    "        .withColumn(\"time\", date_format(\"timestamp\", \"HH:mm\")) \\\n",
    "        .select(\"date\",\"time\",\"SCRAMBLED_IMSI\", \"Download_Type\", \"MCC\", \"MNC\", \"LAC\", \"SAC\", \"TAC\",\n",
    "                    \"E_NODE_B_ID\", \"SECTOR_ID\", \"RAT\", \"NEW_CALL_ATTEMPTS\", \"NEW_ANSWERED_CALLS\",\n",
    "                    \"NEW_FAILED_CALLS\", \"NEW_DROPPED_CALLS\", \"NEW_PS_ATTEMPTS\", \"NEW_FAILED_PS\", \"NEW_DROPPED_PS\",\n",
    "                    \"NEW_SMS_ATTEMPTS\", \"NEW_FAILED_SMS\", \"NEW_LU_ATTEMPTS\", \"NEW_FAILED_LU\", \"NEW_RAUTAU_ATTEMPTS\",\n",
    "                    \"NEW_FAILED_RAUTAU\", \"NEW_ATTACH_ATTEMPTS\", \"NEW_FAILED_ATTACH\", \"NEW_DETACH_ATTEMPTS\",\n",
    "                    \"NEW_DL_VOLUME\", \"NEW_DL_MAX_THROUGHPUT\", \"NEW_UL_VOLUME\", \"NEW_UL_MAX_THROUGHPUT\")\n",
    "        \n",
    "\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 113/113 [00:07<00:00, 15.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Runs slow first time but after data caching findData takes only seconds\n",
    "\n",
    "import cProfile\n",
    "import time\n",
    "#cProfile.run('__findData(\"2022-05-01\", \"2022-12-01\", \"mon\", \"09-12, 13-15\")')\n",
    "\n",
    "files = __findData(\"2022-01-01\", \"2022-12-01\", \"wed\", \"00-24\")\n",
    "\n",
    "df = __ingest(files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "551354102"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[1].count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
